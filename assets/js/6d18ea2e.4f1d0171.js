"use strict";(self.webpackChunkdocusaurus_yt_example=self.webpackChunkdocusaurus_yt_example||[]).push([[5301],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=r.createContext({}),p=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=p(e.components);return r.createElement(c.Provider,{value:t},e.children)},l="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),l=p(n),h=o,d=l["".concat(c,".").concat(h)]||l[h]||m[h]||a;return n?r.createElement(d,s(s({ref:t},u),{},{components:n})):r.createElement(d,s({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,s=new Array(a);s[0]=h;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[l]="string"==typeof e?e:o,s[1]=i;for(var p=2;p<a;p++)s[p]=n[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},4839:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>p});var r=n(7462),o=(n(7294),n(3905));const a={},s="Building a Chatbot with LLMs",i={unversionedId:"chatbotllm/chatbotLLM",id:"chatbotllm/chatbotLLM",title:"Building a Chatbot with LLMs",description:"Now, we'll build a chatbot app, powered by an open source LLM.",source:"@site/docs/chatbotllm/chatbotLLM.md",sourceDirName:"chatbotllm",slug:"/chatbotllm/",permalink:"/khacks-gen-ai/docs/chatbotllm/",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/chatbotllm/chatbotLLM.md",tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Creating an Image Generation Application using Clipdrop API and Gradio",permalink:"/khacks-gen-ai/docs/imagegen/clipdropgradio"}},c={},p=[{value:"IMPORTING APIS AND HELPER FUNCTION",id:"importing-apis-and-helper-function",level:2},{value:"Building an app to chat with any LLM",id:"building-an-app-to-chat-with-any-llm",level:3},{value:"Adding other advanced features",id:"adding-other-advanced-features",level:3}],u={toc:p},l="wrapper";function m(e){let{components:t,...n}=e;return(0,o.kt)(l,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"building-a-chatbot-with-llms"},"Building a Chatbot with LLMs"),(0,o.kt)("p",null,"Now, we'll build a chatbot app, powered by an open source LLM.\nYou probably already chatted with ChatGPT, but running it can be costly and\nrigid. Custom LLMs can run locally, be fine-tuned in your own data, or run\ncheaper on the cloud.\nHere, we'll be using an inference endpoint running \"falcon-40B-Instruct\", one of the best open-source large language models."),(0,o.kt)("h2",{id:"importing-apis-and-helper-function"},"IMPORTING APIS AND HELPER FUNCTION"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"#apikey\nimport os\nimport io\nimport IPython.display\nfrom PIL import Image\nimport base64\nimport requests\nrequests.adapters.DEFAULT_TIMEOUT = 60\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nhf_api_key = os.environ['HF_API_KEY']\n# Helper function\nimport requests, json\nfrom text_generation import Client\n#FalcomLM-instruct endpoint on the text_generation library\nclient = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\":\nf\"Basic {hf_api_key}\"}, timeout=120)\n")),(0,o.kt)("h3",{id:"building-an-app-to-chat-with-any-llm"},"Building an app to chat with any LLM"),(0,o.kt)("p",null,"Here we'll be using an Inference Endpoint for falcon-40b-instruct , one of best ranking open source LLM on the Open LLM Leaderboard."),(0,o.kt)("p",null,"To run it locally, one can use the Transformers library or the text-generationinference"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Transformers Library: Transformers provides APIs and tools to easily\ndownload and train state-of-the-art pretrained models. Using pretrained\nmodels can reduce your compute costs, carbon footprint, and save you\nthe time and resources required to train a model from scratch. These\nmodels support common tasks in different modalities, such as Computer\nvision, Natural Language Processing etc"),(0,o.kt)("li",{parentName:"ul"},"Text generation inference github link -\n",(0,o.kt)("a",{parentName:"li",href:"https://github.com/huggingface/text-generation-inference"},"https://github.com/huggingface/text-generation-inference"),"\nSource Code:")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# This section sets up an initial prompt and uses a language model (referred to as client) to generate text based on the prompt. The generated text is not stored or used further in the code.\nprompt = "Has math been invented or discovered?"\nclient.generate(prompt, max_new_tokens=256).generated_text\n# This defines a function respond that simulates a simple chatbot response. It takes a user message (message) and a history of the conversation (chat_history) as input. The chatbot randomly selects one of three pre-made responses, appends the user message and bot message to the chat history, and returns an updated chat history.\nimport random\ndef respond(message, chat_history):\n #No LLM here, just respond with a random pre-made message\n bot_message = random.choice(["Tell me more about it", "Cool, but I\'m not interested", "Hmmmm, ok then"])\n chat_history.append((message, bot_message))\n return "", chat_history\n# This section seems to define a graphical user interface (GUI) using the Blocks library. It creates a chatbot interface with a textbox for user input (msg), a submit button (btn), and a clear button (clear).\nwith gr.Blocks() as demo:\n chatbot = gr.Chatbot(height=240) #just to fit the notebook\n msg = gr.Textbox(label="Prompt")\n btn = gr.Button("Submit")\n clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")\n btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\ndemo.launch(share=True, server_port=int(os.environ[\'PORT2\u2019]))\ndef format_chat_prompt(message, chat_history):\n prompt = ""\n for turn in chat_history:\n user_message, bot_message = turn\n prompt = f"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}"\n prompt = f"{prompt}\\nUser: {message}\\nAssistant:"\n return prompt\ndef respond(message, chat_history):\n formatted_prompt = format_chat_prompt(message, chat_history)\n bot_message = client.generate(formatted_prompt,\n max_new_tokens=1024,\n stop_sequences=["\\nUser:",\n"<|endoftext|>"]).generated_text\n chat_history.append((message, bot_message))\n return "", chat_history\nwith gr.Blocks() as demo:\n chatbot = gr.Chatbot(height=240) #just to fit the notebook\n msg = gr.Textbox(label="Prompt")\n btn = gr.Button("Submit")\n clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")\n btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\ndemo.launch(share=True, server_port=int(os.environ[\'PORT3\']))\n')),(0,o.kt)("h3",{id:"adding-other-advanced-features"},"Adding other advanced features"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# This function, format_chat_prompt, takes three inputs: message (the user\'s current input), chat_history (the conversation history), and instruction (a system message to be included in the prompt). It creates a formatted prompt string by combining the system instruction, user messages, and assistant messages from the chat history\ndef format_chat_prompt(message, chat_history, instruction):\n prompt = f"System:{instruction}"\n for turn in chat_history:\n user_message, bot_message = turn\n prompt = f"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}"\n prompt = f"{prompt}\\nUser: {message}\\nAssistant:"\n return prompt\n#This code setups a graphical user interface for the advanced features of the chatbot\ndef respond(message, chat_history, instruction, temperature=0.7):\n prompt = format_chat_prompt(message, chat_history, instruction)\n chat_history = chat_history + [[message, ""]]\n stream = client.generate_stream(prompt,\n max_new_tokens=1024,\n stop_sequences=["\\nUser:", ""],\n temperature=temperature)\n #stop_sequences to not generate the user answer\n acc_text = ""\n #Streaming the tokens\n for idx, response in enumerate(stream):\n text_token = response.token.text\n if response.details:\n return\n if idx == 0 and text_token.startswith(" "):\n text_token = text_token[1:]\n acc_text += text_token\n last_turn = list(chat_history.pop(-1))\n last_turn[-1] += acc_text\n chat_history = chat_history + [last_turn]\n yield "", chat_history\n acc_text = ""\n# This code snippet appears to be creating a graphical user interface (GUI) using the Blocks library to facilitate user interactions with a chatbot.\nwith gr.Blocks() as demo:\n chatbot = gr.Chatbot(height=240) #just to fit the notebook\n msg = gr.Textbox(label="Prompt")\n with gr.Accordion(label="Advanced options", open=False):\n system = gr.Textbox(label="System message", lines=2, value="A\nconversation between a user and an LLM-based AI assistant. The assistant gives\nhelpful and honest answers.")\n temperature = gr.Slider(label="temperature", minimum=0.1, maximum=1,\nvalue=0.7, step=0.1)\n btn = gr.Button("Submit")\n clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")\n btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg,\nchatbot]) #Press enter to submit\ngr.close_all()\ndemo.queue().launch(share=True, server_port=int(os.environ[\'PORT4\']))\n')),(0,o.kt)("p",null,"This is how you build a chatbot using LLM"))}m.isMDXComponent=!0}}]);