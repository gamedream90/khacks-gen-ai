"use strict";(self.webpackChunkdocusaurus_yt_example=self.webpackChunkdocusaurus_yt_example||[]).push([[1217],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),m=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=m(n),c=o,d=p["".concat(l,".").concat(c)]||p[c]||h[c]||r;return n?a.createElement(d,i(i({ref:t},u),{},{components:n})):a.createElement(d,i({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:o,i[1]=s;for(var m=2;m<r;m++)i[m]=n[m];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},7517:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>m});var a=n(7462),o=(n(7294),n(3905));const r={},i="Langchain",s={unversionedId:"langchain/intro",id:"langchain/intro",title:"Langchain",description:"What is Langchain",source:"@site/docs/langchain/intro.md",sourceDirName:"langchain",slug:"/langchain/intro",permalink:"/khacks-gen-ai/docs/langchain/intro",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/langchain/intro.md",tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Building Chat Apps with Cohere",permalink:"/khacks-gen-ai/docs/chatbotwithCohert/intro"},next:{title:"Chat with PDF",permalink:"/khacks-gen-ai/docs/chatwithpdf/intro"}},l={},m=[{value:"Program",id:"program",level:2},{value:"Explanation",id:"explanation",level:2},{value:"What is ConversationBufferMemory?",id:"what-is-conversationbuffermemory",level:2},{value:"What is Conversation Buffer Window Memory?",id:"what-is-conversation-buffer-window-memory",level:2},{value:"What is ConversationTokenBufferMemory?",id:"what-is-conversationtokenbuffermemory",level:2}],u={toc:m},p="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"langchain"},"Langchain"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"What is Langchain")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"It is designed to make it easier to connect LLMs with other data sources and allow the LLM to take actions."),(0,o.kt)("li",{parentName:"ul"},"It provides standard interfaces for concepts like chains, agents, memory, and indexes. Chains allow sequencing multiple LLM calls, agents involve the LLM taking actions, memory persists state across calls, and indexes connect the LLM to custom text data."),(0,o.kt)("li",{parentName:"ul"},"It has modules for common use cases like personal assistants, question answering, chatbots, querying data, interacting with APIs, summarization, and more."),(0,o.kt)("li",{parentName:"ul"},"The goal is to simplify the process of building more advanced applications with LLMs beyond just a single API call.")),(0,o.kt)("h2",{id:"program"},"Program"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\nfrom langchain.llms import Cohere\nfrom langchain import PromptTemplate, LLMChain\n\n\ntemplate = """Question: {question}\n\n\nAnswer: Let\'s think step by step."""\n\n\nprompt = PromptTemplate(template=template, input_variables=["question"] )\n\n\nllm = Cohere(cohere_api_key="YOUR_API_KEY_HERE\u201d)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = "What NFL team won the Super Bowl in the year Justin Beiber was born?"\nresponse = llm_chain.run(question) \nprint(response)\n')),(0,o.kt)("h2",{id:"explanation"},"Explanation"),(0,o.kt)("p",null,"This code is using the langchain library to generate a response to a given question using the Cohere language model. Here is an explanation of the code line by line: /n/n\n",(0,o.kt)("strong",{parentName:"p"},"import os:")," This line imports the os module, which provides a way of using operating system dependent functionality. /n/n\n",(0,o.kt)("strong",{parentName:"p"},"from dotenv import load_dotenv, find_dotenv:")," This line imports the load_dotenv and find_dotenv functions from the dotenv module. These functions are used to load environment variables from a .env file."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"_ = load_dotenv(find_dotenv()):")," This line calls the load_dotenv function with the result of the find_dotenv function as its argument. This loads the environment variables from the .env file into the current environment."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"import warnings:")," This line imports the warnings module, which provides a way to issue warning messages.\nwarnings.filterwarnings('ignore'): This line calls the filterwarnings function with the argument 'ignore'. This suppresses all warning messages."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"from langchain.llms import Cohere:")," This line imports the Cohere class from the langchain.llms module. The Cohere class is used to interact with the Cohere language model."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"from langchain import PromptTemplate, LLMChain:")," This line imports the PromptTemplate and LLMChain classes from the langchain module. The PromptTemplate class is used to define a template for generating prompts for the language model, and the LLMChain class is used to chain multiple language models together."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},'template = """Question:'),' {question}/n/n/nAnswer: Let\'s think step by step.""": This line defines a template string for generating prompts for the language model. The template includes placeholders for input variables, which will be replaced with their values when generating a prompt.'),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"prompt = PromptTemplate(template=template, input_variables=",'["question"]',"):")," This line creates an instance of the PromptTemplate class with the previously defined template string and a list of input variables. /n/n\n",(0,o.kt)("strong",{parentName:"p"},'llm = Cohere(cohere_api_key="YOUR_API_KEY_HERE\u201d):')," This line creates an instance of the Cohere class with an API key for accessing the Cohere language model."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"llm_chain = LLMChain(prompt=prompt, llm=llm):")," This line creates an instance of the LLMChain class with the previously created instances of the PromptTemplate and Cohere classes."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},'question = "What NFL team won the Super Bowl in the year Justin Beiber was born?":')," This line defines a question string that will be used as input to the language model."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"response = llm_chain.run(question):")," This line calls the run method on the previously created instance of the LLMChain class with the question string as its argument. This generates a response to the question using the Cohere language model. "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"print(response):")," This line prints out the generated response."),(0,o.kt)("p",null,"I hope this helps! Let me know if you have any further questions \ud83d\ude0a"),(0,o.kt)("h2",{id:"what-is-conversationbuffermemory"},"What is ConversationBufferMemory?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"It stores the conversation history as a string containing the recent input/output pairs."),(0,o.kt)("li",{parentName:"ul"},"The full conversation history is passed to the LLM each time, so the LLM has context on the full conversation."),(0,o.kt)("li",{parentName:"ul"},"It does not limit the number of interactions stored, so the memory buffer can grow very large over time."),(0,o.kt)("li",{parentName:"ul"},"It is useful when you want the full context for the LLM, without any truncation of history."),(0,o.kt)("li",{parentName:"ul"},"The memory buffer is flushed when .reset() is called on the memory.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Program")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.memory import ConversationBufferMemory\nllm = ChatOpenAI(temperature=0.0)\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=True\n\n\nconversation.predict(input="Hi, my name is Andrew")\nconversation.predict(input="What is 1+1?")\nconversation.predict(input="What is my name?")\nprint(memory.buffer)\nmemory.load_memory_variables({})\nmemory = ConversationBufferMemory()\nmemory.save_context({"input": "Hi"}, \n                    {"output": "What\'s up"})\nprint(memory.buffer)\nmemory.load_memory_variables({})\nmemory.save_context({"input": "Not much, just hanging"}, \n                    {"output": "Cool"})\nmemory.load_memory_variables({})\n\n')),(0,o.kt)("h2",{id:"what-is-conversation-buffer-window-memory"},"What is Conversation Buffer Window Memory?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"It maintains a list of the past interactions in a conversation."),(0,o.kt)("li",{parentName:"ul"},"Only the most recent K interactions are kept, where K is a parameter that can be set. Older interactions are discarded."),(0,o.kt)("li",{parentName:"ul"},"This keeps the memory buffer from growing too large, while still providing context from recent interactions."),(0,o.kt)("li",{parentName:"ul"},"The memory can return the interactions as raw text or as structured messages."),(0,o.kt)("li",{parentName:"ul"},"It's useful for chat models where you want to limit the context to the most recent exchanges.")),(0,o.kt)("h2",{id:"what-is-conversationtokenbuffermemory"},"What is ConversationTokenBufferMemory?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"It stores a buffer of the most recent messages between a human and AI."),(0,o.kt)("li",{parentName:"ul"},"It uses token length rather than number of messages to determine when to flush old interactions from the buffer."),(0,o.kt)("li",{parentName:"ul"},"You specify the max token limit when initializing the memory. Once that limit is exceeded, old messages will be removed from the buffer."),(0,o.kt)("li",{parentName:"ul"},"The messages are stored as a string called 'history' that can be loaded into the prompt to give the AI conversational context."),(0,o.kt)("li",{parentName:"ul"},"You can also get the messages as a list, which is useful if you want to provide context to a chat model."),(0,o.kt)("li",{parentName:"ul"},"It's designed to be used with ConversationChain to provide a context buffer for conversational AI systems.\nWhat is ConversationSummaryMemory?"),(0,o.kt)("li",{parentName:"ul"},"It creates a summary of the conversation as it happens and stores the current summary in memory."),(0,o.kt)("li",{parentName:"ul"},"This summary can then be injected into a prompt or chain to provide context about the conversation so far."),(0,o.kt)("li",{parentName:"ul"},"It is useful for longer conversations where keeping the full message history would take up too many tokens."),(0,o.kt)("li",{parentName:"ul"},"It has methods like save_context to save new messages, load_memory_variables to load the current summary, and predict_new_summary to generate a new summary."),(0,o.kt)("li",{parentName:"ul"},"It can be initialized directly with a ChatMessageHistory object containing existing messages."),(0,o.kt)("li",{parentName:"ul"},"The summary allows condensing information from the conversation over time rather than keeping the full verbatim history."),(0,o.kt)("li",{parentName:"ul"},"When used in a ConversationChain, it will update the summary after each new user input.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Program")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.memory import ConversationSummaryBufferMemory\n# create a long string\nschedule = "There is a meeting at 8am with your product team. \\\nYou will need your powerpoint presentation prepared. \\\n9am-12pm have time to work on your LangChain \\\nproject which will go quickly because Langchain is such a powerful tool. \\\nAt Noon, lunch at the italian resturant with a customer who is driving \\\nfrom over an hour away to meet you to understand the latest in AI. \\\nBe sure to bring your laptop to show the latest LLM demo."\n\n\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\nmemory.save_context({"input": "Hello"}, {"output": "What\'s up"})\nmemory.save_context({"input": "Not much, just hanging"},\n                    {"output": "Cool"})\nmemory.save_context({"input": "What is on the schedule today?"}, \n                    {"output": f"{schedule}"})\nmemory.load_memory_variables({})\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=True\n)\nconversation.predict(input="What would be a good demo to show?")\nmemory.load_memory_variables({})\n')))}h.isMDXComponent=!0}}]);